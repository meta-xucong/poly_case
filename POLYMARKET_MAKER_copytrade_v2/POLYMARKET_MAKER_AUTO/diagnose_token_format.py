#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯Šæ–­token_idæ ¼å¼åŒ¹é…é—®é¢˜
æ£€æŸ¥ws_cache.jsonä¸­çš„token_idæ ¼å¼ä¸å®é™…æŸ¥è¯¢çš„æ ¼å¼æ˜¯å¦ä¸€è‡´
"""

import json
from pathlib import Path

SCRIPT_DIR = Path(__file__).parent

def diagnose_token_format():
    """è¯Šæ–­token_idæ ¼å¼é—®é¢˜"""
    print("=" * 80)
    print("Token ID æ ¼å¼åŒ¹é…è¯Šæ–­")
    print("=" * 80)

    # 1. è¯»å–ws_cache.json
    cache_path = SCRIPT_DIR / "ws_cache.json"
    if not cache_path.exists():
        print(f"âŒ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_path}")
        return

    try:
        with open(cache_path, "r", encoding="utf-8") as f:
            payload = json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return

    tokens = payload.get("tokens", {})
    if not tokens:
        print("âŒ ç¼“å­˜ä¸­æ²¡æœ‰tokensæ•°æ®")
        return

    print(f"âœ… ç¼“å­˜ä¸­æœ‰ {len(tokens)} ä¸ªtoken\n")

    # 2. è¯»å–tokens_from_copytrade.jsonè·å–æœŸæœ›çš„token_idåˆ—è¡¨
    copytrade_path = SCRIPT_DIR / "tokens_from_copytrade.json"
    expected_tokens = []

    if copytrade_path.exists():
        try:
            with open(copytrade_path, "r", encoding="utf-8") as f:
                copytrade_data = json.load(f)
                expected_tokens = copytrade_data.get("token_ids", [])
            print(f"âœ… æœŸæœ›è®¢é˜… {len(expected_tokens)} ä¸ªtoken\n")
        except Exception as e:
            print(f"âš ï¸  è¯»å–copytradeé…ç½®å¤±è´¥: {e}\n")

    # 3. åˆ†æç¼“å­˜ä¸­çš„token_idæ ¼å¼
    print("ğŸ“Š ç¼“å­˜ä¸­çš„token_idæ ¼å¼åˆ†æ:")
    print("-" * 80)

    cache_tokens = list(tokens.keys())
    for i, token_id in enumerate(cache_tokens[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
        token_data = tokens[token_id]
        seq = token_data.get("seq", "N/A")
        price = token_data.get("price", "N/A")

        print(f"\n[Token {i}]")
        print(f"  ID: {token_id}")
        print(f"  ç±»å‹: {type(token_id)}")
        print(f"  é•¿åº¦: {len(token_id)}")
        print(f"  Seq: {seq}")
        print(f"  Price: {price}")

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­—å­—ç¬¦ä¸²
        if token_id.isdigit():
            print(f"  âœ“ æ ¼å¼: çº¯æ•°å­—å­—ç¬¦ä¸²")
        else:
            print(f"  âœ— æ ¼å¼: åŒ…å«éæ•°å­—å­—ç¬¦")

    if len(cache_tokens) > 10:
        print(f"\n... (è¿˜æœ‰ {len(cache_tokens)-10} ä¸ªtoken)")

    # 4. æ£€æŸ¥æœŸæœ›tokenæ˜¯å¦åœ¨ç¼“å­˜ä¸­
    if expected_tokens:
        print(f"\n{'='*80}")
        print("ğŸ” æ ¼å¼åŒ¹é…æ£€æŸ¥:")
        print("-" * 80)

        for i, expected_id in enumerate(expected_tokens[:5], 1):  # åªæ£€æŸ¥å‰5ä¸ª
            # å°è¯•å¤šç§æ ¼å¼
            formats_to_try = [
                str(expected_id),
                str(int(expected_id)) if str(expected_id).isdigit() else expected_id,
                expected_id.strip() if isinstance(expected_id, str) else str(expected_id),
            ]

            print(f"\n[æœŸæœ›Token {i}]")
            print(f"  åŸå§‹å€¼: {expected_id}")
            print(f"  åŸå§‹ç±»å‹: {type(expected_id)}")

            found = False
            for fmt in formats_to_try:
                if fmt in tokens:
                    print(f"  âœ… æ‰¾åˆ°åŒ¹é…: æ ¼å¼='{fmt}', ç±»å‹={type(fmt)}")
                    found = True
                    break

            if not found:
                print(f"  âŒ æœªæ‰¾åˆ°åŒ¹é…")
                print(f"  å°è¯•è¿‡çš„æ ¼å¼:")
                for fmt in formats_to_try:
                    print(f"    - '{fmt}' (ç±»å‹={type(fmt).__name__})")

                # æ¨¡ç³ŠåŒ¹é…ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ç›¸ä¼¼çš„key
                similar_keys = [k for k in cache_tokens if expected_id in k or k in str(expected_id)]
                if similar_keys:
                    print(f"  ğŸ’¡ å‘ç°ç›¸ä¼¼çš„key:")
                    for key in similar_keys[:3]:
                        print(f"    - '{key}'")

    # 5. æ˜¾ç¤ºå»ºè®®
    print(f"\n{'='*80}")
    print("ğŸ’¡ è¯Šæ–­å»ºè®®:")
    print("-" * 80)

    # æ£€æŸ¥token_idæ ¼å¼ä¸€è‡´æ€§
    all_numeric = all(k.isdigit() for k in cache_tokens)
    if all_numeric:
        print("  âœ“ æ‰€æœ‰ç¼“å­˜token_idéƒ½æ˜¯çº¯æ•°å­—å­—ç¬¦ä¸²æ ¼å¼")
    else:
        print("  âš ï¸  ç¼“å­˜ä¸­çš„token_idæ ¼å¼ä¸ä¸€è‡´")

    # æ£€æŸ¥é•¿åº¦
    lengths = set(len(k) for k in cache_tokens)
    if len(lengths) == 1:
        print(f"  âœ“ æ‰€æœ‰token_idé•¿åº¦ä¸€è‡´: {list(lengths)[0]} å­—ç¬¦")
    else:
        print(f"  âš ï¸  token_idé•¿åº¦ä¸ä¸€è‡´: {lengths}")

    print("\n  å»ºè®®:")
    print("  1. ç¡®ä¿child processæŸ¥è¯¢æ—¶ä½¿ç”¨ str(token_id)")
    print("  2. æ£€æŸ¥token_idæ˜¯å¦è¢«æ„å¤–æˆªæ–­æˆ–ä¿®æ”¹")
    print("  3. åœ¨_load_shared_ws_snapshot()ä¸­æ·»åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—")

if __name__ == "__main__":
    try:
        diagnose_token_format()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  è¯Šæ–­ä¸­æ–­")
